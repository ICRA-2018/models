{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Contrastive Networks\n",
    "\n",
    "This implements [\"Time Contrastive Networks\"](https://arxiv.org/abs/1704.06888),\n",
    "which is part of the larger [Self-Supervised Imitation\n",
    "Learning](https://sermanet.github.io/imitation/) project.\n",
    "\n",
    "![](https://sermanet.github.io/tcn/docs/figs/mvTCN.png)\n",
    "\n",
    "## Contacts\n",
    "\n",
    "Maintainers of TCN:\n",
    "\n",
    "*   Corey Lynch: [github](https://github.com/coreylynch),\n",
    "    [twitter](https://twitter.com/coreylynch)\n",
    "*   Pierre Sermanet: [github](https://github.com/sermanet),\n",
    "    [twitter](https://twitter.com/psermanet)\n",
    "\n",
    "## Contents\n",
    "\n",
    "*   [Getting Started](#getting-started)\n",
    "    *   [Install Dependencies](#install-dependencies)\n",
    "    *   [Download the Inception v3\n",
    "        Checkpoint](#download-pretrained-inceptionv3-checkpoint)\n",
    "    *   [Run all the tests](#run-all-the-tests)\n",
    "*   [Concepts](#concepts)\n",
    "    *   [Multi-view Webcam Video](#multi-view-webcam-video)\n",
    "    *   [Data Pipelines](#data-pipelines)\n",
    "    *   [Estimators](#estimators)\n",
    "    *   [Models](#models)\n",
    "    *   [Losses](#losses)\n",
    "    *   [Inference](#inference)\n",
    "    *   [Configuration](#configuration)\n",
    "    *   [Monitoring Training](#monitoring-training)\n",
    "        *   [KNN Classification Error](#knn-classification-error)\n",
    "        *   [KNN Classification Error](#multi-view-alignment)\n",
    "    *   [Visualization](#visualization)\n",
    "        *   [Nearest Neighbor Imitation\n",
    "            Videos](#nearest-neighbor-imitation-videos)\n",
    "        *   [PCA & T-SNE Visualization](#pca-t-sne-visualization)\n",
    "*   [Tutorial Part I: Collecting Multi-View Webcam\n",
    "    Videos](#tutorial-part-i-collecting-multi-view-webcam-videos)\n",
    "    *   [Collect Webcam Videos](#collect-webcam-videos)\n",
    "    *   [Create TFRecords](#create-tfrecords)\n",
    "*   [Tutorial Part II: Training, Evaluation, and\n",
    "    Visualization](#tutorial-part-ii-training-evaluation-and-visualization)\n",
    "    *   [Download Data](#download-data)\n",
    "    *   [Download the Inception v3\n",
    "        Checkpoint](#download-pretrained-inceptionv3-checkpoint)\n",
    "    *   [Define a Config](#define-a-config)\n",
    "    *   [Train](#train)\n",
    "    *   [Evaluate](#evaluate)\n",
    "    *   [Monitor training](#monior-training)\n",
    "    *   [Visualize](#visualize)\n",
    "        *   [Generate Imitation Videos](#generate-imitation-videos)\n",
    "        *   [Run PCA & T-SNE Visualization](#t-sne-pca-visualization)\n",
    "\n",
    "## Getting started\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "*   [Tensorflow nightly build](https://pypi.python.org/pypi/tf-nightly-gpu) or\n",
    "    via `pip install tf-nightly-gpu`.\n",
    "*   [Bazel](http://bazel.io/docs/install.html)\n",
    "*   matplotlib\n",
    "*   sklearn\n",
    "*   opencv\n",
    "\n",
    "### Download Pretrained InceptionV3 Checkpoint\n",
    "\n",
    "Run the script that downloads the pretrained InceptionV3 checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tensorflow-models/tcn\n",
    "python download_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel test :all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "### Multi-View Webcam Video\n",
    "\n",
    "We provide utilities to collect your own multi-view videos in dataset/webcam.py.\n",
    "See the [webcam tutorial](#tutorial-part-i-collecting-multi-view-webcam-videos)\n",
    "for an end to end example of how to collect multi-view webcam data and convert\n",
    "it to the TFRecord format expected by this library.\n",
    "\n",
    "## Data Pipelines\n",
    "\n",
    "We use the [tf.data.Dataset\n",
    "API](https://www.tensorflow.org/guide/datasets) to construct input\n",
    "pipelines that feed training, evaluation, and visualization. These pipelines are\n",
    "defined in `data_providers.py`.\n",
    "\n",
    "## Estimators\n",
    "\n",
    "We define training, evaluation, and inference behavior using the\n",
    "[tf.estimator.Estimator\n",
    "API](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator). See\n",
    "`estimators/mvtcn_estimator.py` for an example of how multi-view TCN training,\n",
    "evaluation, and inference is implemented.\n",
    "\n",
    "## Models\n",
    "\n",
    "Different embedder architectures are implemented in model.py. We used the\n",
    "`InceptionConvSSFCEmbedder` in the pouring experiments, but we're also\n",
    "evaluating `Resnet` embedders.\n",
    "\n",
    "## Losses\n",
    "\n",
    "We use the\n",
    "[tf.contrib.losses.metric_learning](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/losses/metric_learning)\n",
    "library's implementations of triplet loss with semi-hard negative mining and\n",
    "npairs loss. In our experiments, npairs loss has better empirical convergence\n",
    "and produces the best qualitative visualizations, and will likely be our choice\n",
    "for future experiments. See the\n",
    "[paper](http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)\n",
    "for details on the algorithm.\n",
    "\n",
    "## Inference\n",
    "\n",
    "We support 3 modes of inference for trained TCN models:\n",
    "\n",
    "*   Mode 1: Input is a tf.Estimator input_fn (see\n",
    "    [this](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict)\n",
    "    for details). Output is an iterator over embeddings and additional metadata.\n",
    "    See `labeled_eval.py` for a usage example.\n",
    "\n",
    "*   Mode 2: Input is a TFRecord or (or list of TFRecords). This returns an\n",
    "    iterator over tuples of (embeddings, raw_image_strings, sequence_name),\n",
    "    where embeddings is the [num views, sequence length, embedding size] numpy\n",
    "    array holding the full embedded sequence (for all views), raw_image_strings\n",
    "    is a [num views, sequence length] string array holding the jpeg-encoded raw\n",
    "    image strings, and sequence_name is the name of the sequence. See\n",
    "    `generate_videos.py` for a usage example.\n",
    "\n",
    "*   Mode 3: Input is a numpy array of size [num images, height, width, num\n",
    "    channels]. This returns a tuple of (embeddings, raw_image_strings), where\n",
    "    embeddings is a 2-D float32 numpy array holding [num_images, embedding_size]\n",
    "    image embeddings, and raw_image_strings is a 1-D string numpy array holding\n",
    "    [batch_size] jpeg-encoded image strings. This can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "images = np.random.uniform(0, 1, size=(batch_size, 1080, 1920, 3))\n",
    "embeddings, _ = estimator.inference(\n",
    "    images, checkpoint_path=checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `estimators/base_estimator.py` for details.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Data pipelines, training, eval, and visualization are all configured using\n",
    "key-value parameters passed as [YAML](https://en.wikipedia.org/wiki/YAML) files.\n",
    "Configurations can be nested, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "learning:\n",
    "  optimizer: 'adam'\n",
    "  learning_rate: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T objects\n",
    "\n",
    "YAML configs are converted to LuaTable-like `T` object (see\n",
    "`utils/luatables.py`), which behave like a python `dict`, but allow you to use\n",
    "dot notation to access (nested) keys. For example we could access the learning\n",
    "rate in the above config snippet via `config.learning.learning_rate`.\n",
    "\n",
    "### Multiple Configs\n",
    "\n",
    "Multiple configs can be passed to the various binaries as a comma separated list\n",
    "of config paths via the `--config_paths` flag. This allows us to specify a\n",
    "default config that applies to all experiments (e.g. how often to write\n",
    "checkpoints, default embedder hyperparams) and one config per experiment holding\n",
    "the just hyperparams specific to the experiment (path to data, etc.).\n",
    "\n",
    "See `configs/tcn_default.yml` for an example of our default config and\n",
    "`configs/pouring.yml` for an example of how we define the pouring experiments.\n",
    "\n",
    "Configs are applied left to right. For example, consider two config files:\n",
    "\n",
    "default.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "learning:\n",
    "  learning_rate: 0.001 # Default learning rate.\n",
    "  optimizer: 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "myexperiment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "learning:\n",
    "  learning_rate: 1.0 # Experiment learning rate (overwrites default).\n",
    "data:\n",
    "  training: '/path/to/myexperiment/training.tfrecord'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel run train.py --config_paths='default.yml,myexperiment.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results in a final merged config called final_training_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "learning:\n",
    "  optimizer: 'adam'\n",
    "  learning_rate: 1.0\n",
    "data:\n",
    "  training: '/path/to/myexperiment/training.tfrecord'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is created automatically and stored in the experiment log directory\n",
    "alongside model checkpoints and tensorboard summaries. This gives us a record of\n",
    "the exact configs that went into each trial.\n",
    "\n",
    "## Monitoring training\n",
    "\n",
    "We usually look at two validation metrics during training: knn classification\n",
    "error and multi-view alignment.\n",
    "\n",
    "### KNN-Classification Error\n",
    "\n",
    "In cases where we have labeled validation data, we can compute the average\n",
    "cross-sequence KNN classification error (1.0 - recall@k=1) over all embedded\n",
    "labeled images in the validation set. See `labeled_eval.py`.\n",
    "\n",
    "### Multi-view Alignment\n",
    "\n",
    "In cases where there is no labeled validation data, we can look at the how well\n",
    "our model aligns multiple views of same embedded validation sequences. That is,\n",
    "for each embedded validation sequence, for all cross-view pairs, we compute the\n",
    "scaled absolute distance between ground truth time indices and knn time indices.\n",
    "See `alignment.py`.\n",
    "\n",
    "## Visualization\n",
    "\n",
    "We visualize the embedding space learned by our models in two ways: nearest\n",
    "neighbor imitation videos and PCA/T-SNE.\n",
    "\n",
    "### Nearest Neighbor Imitation Videos\n",
    "\n",
    "One of the easiest way to evaluate the understanding of your model is to see how\n",
    "well the model can semantically align two videos via nearest neighbors in\n",
    "embedding space.\n",
    "\n",
    "Consider the case where we have multiple validation demo videos of a human or\n",
    "robot performing the same task. For example, in the pouring experiments, we\n",
    "collected many different multiview validation videos of a person pouring the\n",
    "contents of one container into another, then setting the container down. If we'd\n",
    "like to see how well our embeddings generalize across viewpoint, object/agent\n",
    "appearance, and background, we can construct what we call \"Nearest Neighbor\n",
    "Imitation\" videos, by embedding some validation query sequence `i` from view 1,\n",
    "and finding the nearest neighbor for each query frame in some embedded target\n",
    "sequence `j` filmed from view 1.\n",
    "[Here's](https://sermanet.github.io/tcn/docs/figs/pouring_human.mov.gif) an\n",
    "example of the final product.\n",
    "\n",
    "See `generate_videos.py` for details.\n",
    "\n",
    "### PCA & T-SNE Visualization\n",
    "\n",
    "We can also embed a set of images taken randomly from validation videos and\n",
    "visualize the embedding space using PCA projection and T-SNE in the tensorboard\n",
    "projector. See `visualize_embeddings.py` for details.\n",
    "\n",
    "## Tutorial Part I: Collecting Multi-View Webcam Videos\n",
    "\n",
    "Here we give an end-to-end example of how to collect your own multiview webcam\n",
    "videos and convert them to the TFRecord format expected by training.\n",
    "\n",
    "Note: This was tested with up to 8 concurrent [Logitech c930e\n",
    "webcams](https://www.logitech.com/en-us/product/c930e-webcam) extended with\n",
    "[Plugable 5 Meter (16 Foot) USB 2.0 Active Repeater Extension\n",
    "Cables](https://www.amazon.com/gp/product/B006LFL4X0/ref=oh_aui_detailpage_o05_s00?ie=UTF8&psc=1).\n",
    "\n",
    "### Collect webcam videos\n",
    "\n",
    "Go to dataset/webcam.py\n",
    "\n",
    "1.  Plug your webcams in and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "ls -ltrh /dev/video*\n",
    "```\n",
    "\n",
    "You should see one device listed per connected webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Define some environment variables describing the dataset you're collecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "dataset=tutorial  # Name of the dataset.\n",
    "mode=train  # E.g. 'train', 'validation', 'test', 'demo'.\n",
    "num_views=2 # Number of webcams.\n",
    "viddir=/tmp/tcn/videos # Output directory for the videos.\n",
    "tmp_imagedir=/tmp/tcn/tmp_images # Temp directory to hold images.\n",
    "debug_vids=1 # Whether or not to generate side-by-side debug videos.\n",
    "export DISPLAY=:0.0  # This allows real time matplotlib display.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Run the webcam.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "bazel build -c opt --copt=-mavx webcam && \\\n",
    "bazel-bin/webcam \\\n",
    "--dataset $dataset \\\n",
    "--mode $mode \\\n",
    "--num_views $num_views \\\n",
    "--tmp_imagedir $tmp_imagedir \\\n",
    "--viddir $viddir \\\n",
    "--debug_vids 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Hit Ctrl-C when done collecting, upon which the script will compile videos\n",
    "    for each view and optionally a debug video concatenating multiple\n",
    "    simultaneous views.\n",
    "\n",
    "5.  If `--seqname` flag isn't set, the script will name the first sequence '0',\n",
    "    the second sequence '1', and so on (meaning you can just keep rerunning step\n",
    "    3.). When you are finished, you should see an output viddir with the\n",
    "    following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "videos/0_view0.mov\n",
    "videos/0_view1.mov\n",
    "...\n",
    "videos/0_viewM.mov\n",
    "videos/1_viewM.mov\n",
    "...\n",
    "videos/N_viewM.mov\n",
    "for N sequences and M webcam views.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFRecords\n",
    "\n",
    "Use `dataset/videos_to_tfrecords.py` to convert the directory of videos into a\n",
    "directory of TFRecords files, one per multi-view sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "viddir=/tmp/tcn/videos\n",
    "dataset=tutorial\n",
    "mode=train\n",
    "videos=$viddir/$dataset\n",
    "\n",
    "bazel build -c opt videos_to_tfrecords && \\\n",
    "bazel-bin/videos_to_tfrecords --logtostderr \\\n",
    "--input_dir $videos/$mode \\\n",
    "--output_dir ~/tcn_data/$dataset/$mode \\\n",
    "--max_per_shard 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `--max_per_shard` > 0 allows you to shard training data. We've observed\n",
    "that sharding long training sequences provides better performance in terms of\n",
    "global steps/sec.\n",
    "\n",
    "This should be left at the default of 0 for validation / test data.\n",
    "\n",
    "You should now have a directory of TFRecords files with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "output_dir/0.tfrecord\n",
    "...\n",
    "output_dir/N.tfrecord\n",
    "\n",
    "1 TFRecord file for each of N multi-view sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to move on to part II: training, evaluation, and visualization.\n",
    "\n",
    "## Tutorial Part II: Training, Evaluation, and Visualization\n",
    "\n",
    "Here we give an end-to-end example of how to train, evaluate, and visualize the\n",
    "embedding space learned by TCN models.\n",
    "\n",
    "### Download Data\n",
    "\n",
    "We will be using the 'Multiview Pouring' dataset, which can be downloaded using\n",
    "the download.sh script\n",
    "[here.](https://sites.google.com/site/brainrobotdata/home/multiview-pouring)\n",
    "\n",
    "The rest of the tutorial will assume that you have your data downloaded to a\n",
    "folder at `~/tcn_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir ~/tcn_data\n",
    "mv ~/Downloads/download.sh ~/tcn_data\n",
    "./download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have the following path containing all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls ~/tcn_data/multiview-pouring\n",
    "labels  README.txt  tfrecords  videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pretrained Inception Checkpoint\n",
    "\n",
    "If you haven't already, run the script that downloads the pretrained InceptionV3\n",
    "checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python download_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define A Config\n",
    "\n",
    "For our experiment, we create 2 configs:\n",
    "\n",
    "*   `configs/tcn_default.yml`: This contains all the default hyperparameters\n",
    "    that generally don't vary across experiments.\n",
    "*   `configs/pouring.yml`: This contains all the hyperparameters that are\n",
    "    specific to the pouring experiment.\n",
    "\n",
    "Important note about `configs/pouring.yml`:\n",
    "\n",
    "*   data.eval_cropping: We use 'pad200' for the pouring dataset, which was\n",
    "    filmed rather close up on iphone cameras. A better choice for data filmed on\n",
    "    webcam is likely 'crop_center'. See preprocessing.py for options.\n",
    "\n",
    "### Train\n",
    "\n",
    "Run the training binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "yaml"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "logdir=/tmp/tcn/pouring\n",
    "c=configs\n",
    "configs=$c/tcn_default.yml,$c/pouring.yml\n",
    "\n",
    "bazel build -c opt --copt=-mavx --config=cuda train && \\\n",
    "bazel-bin/train \\\n",
    "--config_paths $configs --logdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Run the binary that computes running validation loss. Set `export\n",
    "CUDA_VISIBLE_DEVICES=` to run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel build -c opt --copt=-mavx eval && \\\n",
    "bazel-bin/eval \\\n",
    "--config_paths $configs --logdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the binary that computes running validation cross-view sequence alignment.\n",
    "Set `export CUDA_VISIBLE_DEVICES=` to run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel build -c opt --copt=-mavx alignment && \\\n",
    "bazel-bin/alignment \\\n",
    "--config_paths $configs --checkpointdir $logdir --outdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the binary that computes running labeled KNN validation error. Set `export\n",
    "CUDA_VISIBLE_DEVICES=` to run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel build -c opt --copt=-mavx labeled_eval && \\\n",
    "bazel-bin/labeled_eval \\\n",
    "--config_paths $configs --checkpointdir $logdir --outdir $logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor training\n",
    "\n",
    "Run `tensorboard --logdir=$logdir`. After a bit of training, you should see\n",
    "curves that look like this:\n",
    "\n",
    "#### Training loss\n",
    "\n",
    "<img src=\"g3doc/loss.png\" title=\"Training Loss\" />\n",
    "\n",
    "#### Validation loss\n",
    "\n",
    "<img src=\"g3doc/val_loss.png\" title=\"Validation Loss\" />\n",
    "\n",
    "#### Validation Alignment\n",
    "\n",
    "<img src=\"g3doc/alignment.png\" title=\"Validation Alignment\" />\n",
    "\n",
    "#### Average Validation KNN Classification Error\n",
    "\n",
    "<img src=\"g3doc/avg_error.png\" title=\"Validation Average KNN Error\" />\n",
    "\n",
    "#### Individual Validation KNN Classification Errors\n",
    "\n",
    "<img src=\"g3doc/all_error.png\" title=\"All Validation Average KNN Errors\" />\n",
    "\n",
    "### Visualize\n",
    "\n",
    "To visualize the embedding space learned by a model, we can:\n",
    "\n",
    "#### Generate Imitation Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Use the automatically generated final config file as config.\n",
    "configs=$logdir/final_training_config.yml\n",
    "# Visualize checkpoint 40001.\n",
    "checkpoint_iter=40001\n",
    "# Use validation records for visualization.\n",
    "records=~/tcn_data/multiview-pouring/tfrecords/val\n",
    "# Write videos to this location.\n",
    "outdir=$logdir/tcn_viz/imitation_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bazel build -c opt --config=cuda --copt=-mavx generate_videos && \\\n",
    "bazel-bin/generate_videos \\\n",
    "--config_paths $configs \\\n",
    "--checkpointdir $logdir \\\n",
    "--checkpoint_iter $checkpoint_iter \\\n",
    "--query_records_dir $records \\\n",
    "--target_records_dir $records \\\n",
    "--outdir $outdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the script completes, you should see a directory of videos with names\n",
    "like:\n",
    "\n",
    "`$outdir/qtrain_clearodwalla_to_clear1_realv1_imtrain_clearsoda_to_white13_realv0.mp4`\n",
    "\n",
    "that look like this: <img src=\"g3doc/im.gif\" title=\"Imitation Video\" />\n",
    "\n",
    "#### T-SNE / PCA Visualization\n",
    "\n",
    "Run the binary that generates embeddings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "outdir=$logdir/tcn_viz/embedding_viz\n",
    "bazel build -c opt --config=cuda --copt=-mavx visualize_embeddings && \\\n",
    "bazel-bin/visualize_embeddings \\\n",
    "--config_paths $configs \\\n",
    "--checkpointdir $logdir \\\n",
    "--checkpoint_iter $checkpoint_iter \\\n",
    "--embedding_records $records \\\n",
    "--outdir $outdir \\\n",
    "--num_embed 1000 \\\n",
    "--sprite_dim 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tensorboard, pointed at the embedding viz output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=$outdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like this in tensorboard.\n",
    "<img src=\"g3doc/pca.png\" title=\"PCA\" />"
   ]
  }
 ],

 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
